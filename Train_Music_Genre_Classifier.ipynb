{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a0ae78-7d7d-4e3d-ab6c-92494a45865d",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a28e9c-fcd1-4ca8-8356-f2e0a2b5329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3bc25-936d-41f9-b529-be0eab6782c9",
   "metadata": {},
   "source": [
    "## Visualizing Single Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7555e3d-3ca9-4c80-841f-458053ed4aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_file_name = \"./blues.00000.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "613cdfc7-4004-4721-a62c-4704a2f21e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y,sr = librosa.load(random_file_name,sr=44100)\n",
    "plt.figure(figsize=(14,5))\n",
    "librosa.display.waveshow(y,sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49502238-906a-464d-9687-54b10e3ffd0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43my\u001b[49m.shape\n",
      "\u001b[31mNameError\u001b[39m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5298ae-a0b9-40aa-88f9-a7d10b61ab0a",
   "metadata": {},
   "source": [
    "## Playing Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a57977b1-2c8e-43a7-b6d2-7d849a91daf8",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(data=y,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "201a6093-cbcb-4eea-8bb7-f287c5e023e9",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c3e5f-12da-4b17-96c8-3ceeda63e2e7",
   "metadata": {},
   "source": [
    "### Doing Visualization on chunks of Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab32add1-c869-4b82-bdae-0c94c19a07b8",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "audio_path = \"./blues.00000.wav\"\n",
    "y, sr = librosa.load(audio_path, sr=None)  # sr=None to keep the original sampling rate\n",
    "\n",
    "# Define the duration of each chunk and overlap\n",
    "chunk_duration = 4  # seconds\n",
    "overlap_duration = 2  # seconds\n",
    "\n",
    "# Convert durations to samples\n",
    "chunk_samples = chunk_duration * sr\n",
    "overlap_samples = overlap_duration * sr\n",
    "\n",
    "# Calculate the number of chunks\n",
    "num_chunks = int(np.ceil((len(y) - chunk_samples) / (chunk_samples - overlap_samples))) + 1\n",
    "\n",
    "# Iterate over each chunk\n",
    "for i in range(num_chunks):\n",
    "    # Calculate start and end indices of the chunk\n",
    "    start = i * (chunk_samples - overlap_samples)\n",
    "    end = start + chunk_samples\n",
    "    \n",
    "    # Extract the chunk of audio\n",
    "    chunk = y[start:end]\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    librosa.display.waveshow(chunk, sr=sr)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343979d5-be08-4363-9415-cee26fa6d513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ff2c331-1ed2-4ed4-9f0b-4df494600671",
   "metadata": {},
   "source": [
    "## Melspectrogram Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "53bb1434-6cbe-45da-bd2e-4ba2af5bce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Melspectrogram of Entire Audio\n",
    "def plot_melespectrogram(y,sr):\n",
    "    #Compute spectrogram\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y,sr=sr)\n",
    "    #Convert to decibels (log scale)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram,ref=np.max)\n",
    "    #Visualize the spectrogram\n",
    "    plt.figure(figsize=(10,4))\n",
    "    librosa.display.specshow(spectrogram_db,sr=sr,x_axis='time',y_axis='mel')\n",
    "    plt.colorbar(format='%2.0f dB')\n",
    "    plt.title(\"Spectrogram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "305ba7bf-9ed7-479c-850b-103180393257",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_file_name = \"./blues.00000.wav\"\n",
    "y,sr = librosa.load(random_file_name,sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9352ce89-8afb-47dc-b00b-63a134a4064f",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_melespectrogram(y,sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e29ab17-dd53-4756-ad3b-2edc6dc77a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_melspectrogram_chunks(y,sr):\n",
    "    #define the duration of each chunk and overlap\n",
    "    chunk_duration = 4\n",
    "    overlap_duration = 2\n",
    "    \n",
    "    #Convert duration to sample\n",
    "    chunk_samples = chunk_duration * sr\n",
    "    overlap_samples = overlap_duration * sr\n",
    "    \n",
    "    #Calculate the number of chunks\n",
    "    num_chunks = int(np.ceil((len(y)-chunk_samples)/(chunk_samples-overlap_samples)))+1\n",
    "    \n",
    "    #iterate over each chunks\n",
    "    for i in range(num_chunks):\n",
    "        #Calculate start and end indices of the chunk\n",
    "        start = i*(chunk_samples-overlap_samples)\n",
    "        end = start+chunk_samples\n",
    "        #Extract the chunk audio\n",
    "        chunk = y[start:end]\n",
    "        #Melspectrogram part\n",
    "        spectrogram = librosa.feature.melspectrogram(y=chunk,sr=sr)\n",
    "        print(spectrogram.shape)\n",
    "        spectrogram_db = librosa.power_to_db(spectrogram,ref=np.max)\n",
    "        #Visualize the spectrogram\n",
    "        plt.figure(figsize=(10,4))\n",
    "        librosa.display.specshow(spectrogram_db,sr=sr,x_axis='time',y_axis='mel')\n",
    "        plt.colorbar(format='%2.0f dB')\n",
    "        plt.title(\"Spectrogram\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47af42fc-08f3-422f-8e80-7d05d7b2cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_file_name = \"./blues.00000.wav\"\n",
    "y,sr = librosa.load(random_file_name,sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d587ae37-8068-4702-9be8-0df6cb91370b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "88058d2d-b599-416d-a865-8c859a482fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_melspectrogram_chunks(y,sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e1b78564-3f9c-42cf-baf6-4d2d8ece853c",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "37155798-6c07-4c9d-a915-d591205ca1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "210*210"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18993010-acbd-44e3-801a-2df47e71716a",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add384ad-4e43-4e4e-aa78-d9f0d82709dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your folder structure\n",
    "data_dir = \"./genres_original\"\n",
    "classes = ['blues', 'classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce865fd2-f018-4f1c-b2a4-6856af19bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.image import resize\n",
    "#Load and preprocess audio data\n",
    "def load_and_preprocess_data(data_dir,classes,target_shape=(150,150)):\n",
    "    data=[]\n",
    "    labels=[]\n",
    "\n",
    "    for i_class,class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir,class_name)\n",
    "        print(\"Processing--\",class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith('.wav'):\n",
    "                file_path = os.path.join(class_dir,filename)\n",
    "                audio_data,sample_rate = librosa.load(file_path,sr=None)\n",
    "                #Performing Preprocessing\n",
    "                #define the duration of each chunk and overlap\n",
    "                chunk_duration = 4\n",
    "                overlap_duration = 2\n",
    "                \n",
    "                #Convert duration to sample\n",
    "                chunk_samples = chunk_duration * sample_rate\n",
    "                overlap_samples = overlap_duration * sample_rate\n",
    "                \n",
    "                #Calculate the number of chunks\n",
    "                num_chunks = int(np.ceil((len(audio_data)-chunk_samples)/(chunk_samples-overlap_samples)))+1\n",
    "                \n",
    "                #iterate over each chunks\n",
    "                for i in range(num_chunks):\n",
    "                    #Calculate start and end indices of the chunk\n",
    "                    start = i*(chunk_samples-overlap_samples)\n",
    "                    end = start+chunk_samples\n",
    "                    #Extract the chunk audio\n",
    "                    chunk = audio_data[start:end]\n",
    "                    #Melspectrogram part\n",
    "                    mel_spectrogram = librosa.feature.melspectrogram(y=chunk,sr=sample_rate)\n",
    "                    #Resize matrix based on provided target shape\n",
    "                    mel_spectrogram = resize(np.expand_dims(mel_spectrogram,axis=-1),target_shape)\n",
    "                    #Append data to list\n",
    "                    data.append(mel_spectrogram)\n",
    "                    labels.append(i_class)\n",
    "    #Return\n",
    "    return np.array(data),np.array(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a0ce4d-8c30-4418-a451-ff69a790ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "data,labels = load_and_preprocess_data(data_dir,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c8ace-7505-46e7-a302-67746f15770c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08ad3cd4-e507-4ed8-b1f9-1a0f482b8a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9171d9ce-b9dc-4f52-9f83-32bc14999197",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac97b05-dde1-4361-8f62-dbc8da191c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "labels = to_categorical(labels,num_classes = len(classes)) # Converting labels to one-hot encoding\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994faf64-3914-4a05-8f12-1e5d9bdce3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7afa5-8307-419a-b998-f8f4d876490f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cde149b2-cc82-47f5-a1f4-cc7faf94d63d",
   "metadata": {},
   "source": [
    "## Splitting of Dataset into Training and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7ba336-7035-4d10-aad3-9a7a8f65bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(data,labels,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05503331-06f7-4b1e-a776-e29b8d6118db",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27197cc9-8636-4f0c-bbff-abe0bf7a89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db6a0cb-ca97-49d1-a9a3-9c20f6c6e18c",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eadab4c7-7e70-4558-ad6f-64bd7f2672db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',input_shape=X_train[0].shape))\n",
    "model.add(Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b8e3dfd-a4ab-4308-88ff-8404ed998315",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=64,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e338b979-4488-4932-be91-a78307468882",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=128,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "211971e3-ea5f-41d1-8f4d-eeccbf7f53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf9695c-3d17-43c8-8c04-3cb15837e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=256,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32464375-ca41-4695-93a1-841c37d0322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=512,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=512,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "401b824a-82a0-410f-aeb1-ae148bc2afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97d86585-3d1a-4546-bd7d-c6b7b48c099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c97990e7-08af-4962-b48a-7d4c4ae18f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=1200,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d30e417c-2836-405c-94ea-16966f7dec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2c4d7f6-44cc-4e40-9e85-102404fa612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer\n",
    "model.add(Dense(units=len(classes),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0faf15e5-6e8b-4db4-ac99-15c6d999d741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed62fcba-23fa-4e5e-948b-2c0d50a362ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9967cbf2-a621-40d2-8227-335ec334e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Training Model\n",
    "training_history = model.fit(X_train,Y_train,epochs=30,batch_size=32,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85bbd014-dadd-45ce-a485-18a9dcb7ef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model.save(\"Trained_model.keras\") #Mac\n",
    "model.save(\"Trained_model.h5\") #Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "439f37a0-a106-4322-b46c-76d2b610f8db",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c156765-ad75-4f09-9154-e3aaf784aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recording History in json\n",
    "import json\n",
    "with open('training_hist.json','w') as f:\n",
    "  json.dump(training_history.history,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ad9f7-c0b1-405a-a306-900768a0d5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42ba6757-8288-4c60-adbd-e3e3a6e9765d",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03ad7c45-5ae0-48d1-806b-7dd453846062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Reloading model variable\n",
    "model = tf.keras.models.load_model(\"Trained_model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "335d7940-9472-49e0-84a5-6e2a027c7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reloading Training history\n",
    "import json\n",
    "with open(\"training_hist.json\",'r') as json_file:\n",
    "    training_history_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00080956-61aa-467e-bfd7-4962c4c8a74c",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46c037-5f12-4f1f-a5c4-0b4eedb2972b",
   "metadata": {},
   "source": [
    "## Model Evaluation - I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1b158f0-5329-44aa-844f-755fbfaccf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Model evaluation on Training set\n",
    "train_loss,train_accuracy = model.evaluate(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "127f71b4-4a18-4d07-8dc0-75a3fd085dee",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss,train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d86770f3-1f67-4217-9b1c-497bb8eeef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Model evaluation on Validation set\n",
    "val_loss,val_accuracy = model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a96f195e-ac49-4ae7-bfe1-5352911a9787",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss,val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e1d1be4-624c-43e3-aab0-f6219e3f5240",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history_data['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef155a-8c2e-4ff5-86f1-06a93fe26083",
   "metadata": {},
   "source": [
    "## Accuracy and Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd125394-d4ca-4da8-87b7-f3b17df8da9c",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualization of Loss\n",
    "epochs = [i for i in range(1,31)]\n",
    "plt.plot(epochs,training_history_data['loss'],label=\"Training Loss\",color='red')\n",
    "plt.plot(epochs,training_history_data['val_loss'],label=\"Validation Loss\",color='blue')\n",
    "plt.xlabel(\"No. of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Visualization of Loss Result\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3da1f8be-0c7c-4244-80c5-ca22fa3a5821",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualization of Accuracy\n",
    "epochs = [i for i in range(1,31)]\n",
    "plt.plot(epochs,training_history_data['accuracy'],label=\"Training Accuracy\",color='red')\n",
    "plt.plot(epochs,training_history_data['val_accuracy'],label=\"Validation Accuracy\",color='blue')\n",
    "plt.xlabel(\"No. of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Visualization of Accuracy Result\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b639b9-c043-467e-9217-f7239ba28944",
   "metadata": {},
   "source": [
    "## Precision,Recall,Confusion Matrix - Model Evaluation-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84b4da1b-4b37-45e1-ac39-af6ed1daf663",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09f43751-4f9c-4b49-a9fc-85c487b2432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d504d7e5-575e-4960-8d8c-a96bca95b974",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fd39b6a-e470-4be4-957f-2d09c078ee48",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_categories = np.argmax(y_pred,axis=1)\n",
    "predicted_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7869a427-bab3-4b7b-89c5-52787b2a1422",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5400e3f4-2902-42b7-a1c3-6b5e4b31c1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02787a73-5908-4a95-8bf6-0838feab4a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_categories = np.argmax(Y_test,axis=1)\n",
    "true_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9975da49-7fe2-46a1-bd68-24655c566bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e3a1d3f-14e7-4d61-babf-679d3dbf9508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cm = confusion_matrix(true_categories,predicted_categories)\n",
    "# Precision Recall F1score\n",
    "print(classification_report(true_categories,predicted_categories,target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade1d18-a8c7-4942-a081-511e20d82d5c",
   "metadata": {},
   "source": [
    "## Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1238cbd4-43ac-412f-8303-f62caf6c6b56",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1ebf63b-7773-48ae-a1d8-fdbf82bafd43",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(cm,annot=True,annot_kws={\"size\":10})\n",
    "plt.xlabel(\"Predicted Class\",fontsize=10)\n",
    "plt.ylabel(\"Actual Class\",fontsize=10)\n",
    "plt.title(\"Music Genre Classification Confusion Matrix\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b67e0-a321-4d98-a3e2-d1b57fb33bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
